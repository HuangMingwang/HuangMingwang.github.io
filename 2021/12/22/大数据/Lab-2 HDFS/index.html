<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Lab-2 HDFS | MingwHuang's Blog</title><meta name="keywords" content="Hadoop"><meta name="author" content="MingwHuang"><meta name="copyright" content="MingwHuang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Lab-2 Hadoop(HDFS)分布式文件系统基本操作一、实验原理分析HDFS是什么HDFS（Hadoop Distributed File System） 是一个高度容错性的系统，适合部署在廉价的机器上。HDFS 能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS 放宽了一部分 POSIX 约束，来实现流式读取文件系统数据的目的。HDFS 在最开始是作为 Apache Nutc">
<meta property="og:type" content="article">
<meta property="og:title" content="Lab-2 HDFS">
<meta property="og:url" content="http://example.com/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-2%20HDFS/index.html">
<meta property="og:site_name" content="MingwHuang&#39;s Blog">
<meta property="og:description" content="Lab-2 Hadoop(HDFS)分布式文件系统基本操作一、实验原理分析HDFS是什么HDFS（Hadoop Distributed File System） 是一个高度容错性的系统，适合部署在廉价的机器上。HDFS 能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS 放宽了一部分 POSIX 约束，来实现流式读取文件系统数据的目的。HDFS 在最开始是作为 Apache Nutc">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2021-12-21T16:45:45.000Z">
<meta property="article:modified_time" content="2022-02-22T13:55:23.948Z">
<meta property="article:author" content="MingwHuang">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-2%20HDFS/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Lab-2 HDFS',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-02-22 21:55:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">40</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">MingwHuang's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Lab-2 HDFS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-21T16:45:45.000Z" title="发表于 2021-12-22 00:45:45">2021-12-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-02-22T13:55:23.948Z" title="更新于 2022-02-22 21:55:23">2022-02-22</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Lab-2 HDFS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Lab-2-Hadoop-HDFS-分布式文件系统基本操作"><a href="#Lab-2-Hadoop-HDFS-分布式文件系统基本操作" class="headerlink" title="Lab-2 Hadoop(HDFS)分布式文件系统基本操作"></a>Lab-2 Hadoop(HDFS)分布式文件系统基本操作</h1><h2 id="一、实验原理分析"><a href="#一、实验原理分析" class="headerlink" title="一、实验原理分析"></a>一、实验原理分析</h2><h3 id="HDFS是什么"><a href="#HDFS是什么" class="headerlink" title="HDFS是什么"></a>HDFS是什么</h3><p>HDFS（Hadoop Distributed File System） 是一个高度容错性的系统，适合部署在廉价的机器上。HDFS 能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS 放宽了一部分 POSIX 约束，来实现流式读取文件系统数据的目的。HDFS 在最开始是作为 Apache Nutch 搜索引擎项目的基础架构而开发的。HDFS 是 Apache Hadoop Core 项目的一部分。</p>
<h4 id="HDFS优点"><a href="#HDFS优点" class="headerlink" title="HDFS优点"></a>HDFS优点</h4><ul>
<li><p>保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。为防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其他某几个主机上。</p>
</li>
<li><p>运行在廉价的机器上。</p>
</li>
<li><p>适合大数据的处理。HDFS会将一个完整的大文件平均分块存储到不同计算机上，默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</p>
</li>
<li><p>流式数据访问，一次写入多次读写，和传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化只能在文件末尾添加</p>
</li>
</ul>
<h4 id="HDFS缺点"><a href="#HDFS缺点" class="headerlink" title="HDFS缺点"></a>HDFS缺点</h4><ul>
<li><p>不适合低延时的数据访问，比如毫秒级的存储数据，是做不到的</p>
</li>
<li><p>无法高效的对大量小文件进行存储</p>
<ul>
<li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存是有限的。</li>
<li>小文件的存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li>
</ul>
</li>
<li><p>不支持并发写入，文件随机修改</p>
<ul>
<li>一个文件只能有一个写，不允许多个线程同时写；</li>
<li>仅支持数据append（追加），不支持文件的随机修改</li>
</ul>
</li>
</ul>
<h3 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h3><p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220103740.png" alt="image-20210628203053925"></p>
<h4 id="HDFS-client"><a href="#HDFS-client" class="headerlink" title="HDFS client"></a>HDFS client</h4><p>我们和HDFS打交道是通过一个client library。无论是读取一个文件还是写一个文件，都是先把数据交给HDFS client，它负责和Name nodes以及Data nodes联系并传输数据。</p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><p>在HDFS中，Namenode 保存了整个文件系统信息，包活文件和文件夹的机构。其实和linux上的很像，HDFS也是把文件和文件夹表示为inode，每个inode有自己的所有者，权限，创建的修改时间等等。HDFS可以存很大的文件，所以每个文件被分为一些data block，存在不同的机器上，name node就负责记录一个文件有哪些data block，以及这些data block分别存放在那些机器上。</p>
<p>Namenode 还负责管理文件系统常用操作，比如创建一个文件，重命名一个文件，创建一个文件夹，重命名一个文件夹等。</p>
<p>当我们通过HDFS client先HDFS读取或者写文件时，所有的都写请求都是先发给Name nodes，它负责创建或者查询一个文件，然后再让HDFS client和 Data nodes联系具体的数据传输。</p>
<h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><p>存储data block的机器叫做Data nodes。在读写过程中，Data nodes负责直接把用户读取的文件block传给client，也负责直接接受用户写的文件。</p>
<p>当我们读取一个文件时：</p>
<ul>
<li>HDFS client 联系 Name nodes，获取文件的 data blocks 组成、以及每个 data block 所在的机器以及具体存放位置；</li>
<li>HDFS client 联系 Data nodes, 进行具体的读写操作；</li>
</ul>
<p>重要的事情说三遍。在读写一个文件时，当我们从 Name nodes 得知应该向哪些 Data nodes 读写之后，我们就直接和 Data node 打交道，不再通过 Name nodes。</p>
<h4 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h4><p>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode 并提供服务。</p>
<p>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode。</p>
<p>在紧急情况下，可辅助恢复NameNode。</p>
<h3 id="HDFS文件写入"><a href="#HDFS文件写入" class="headerlink" title="HDFS文件写入"></a>HDFS文件写入</h3><p>Client向NameNode发起文件写入的请求。</p>
<ol>
<li>NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。</li>
<li>Client将文件划分为多个block块，并根据DataNode的地址信息，按顺序写入到每一个DataNode块中。</li>
</ol>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220103741.png" alt="58qho28cfd"></p>
<p>例如：有一个文件FileA，100M大小。Client将FileA写入到HDFS上。</p>
<ol>
<li>HDFS按默认配置。</li>
<li>HDFS分布在三个机架上Rack1，Rack2，Rack3。</li>
</ol>
<p>文件写入过程如下：</p>
<ol>
<li>Client将FileA按64M分块。分成两块，block1和Block2;</li>
<li>Client向NameNode发送写数据请求，如图蓝色虚线①——&gt;。</li>
<li>NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。<ol>
<li>Block1: host2,host1,host3</li>
<li>Block2: host7,host8,host4</li>
<li>原理：<ol>
<li>NameNode具有RackAware机架感知功能，这个可以配置。</li>
<li>若Client为DataNode节点，那存储block时，规则为：副本1，同Client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。</li>
<li>若Client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。</li>
</ol>
</li>
</ol>
</li>
<li>Client向DataNode发送block1；发送过程是以流式写入。流式写入过程如下：<ol>
<li>将64M的block1按64k的package划分;</li>
<li>然后将第一个package发送给host2;</li>
<li>host2接收完后，将第一个package发送给host1，同时Client向host2发送第二个package；</li>
<li>host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。</li>
<li>以此类推，如图红线实线所示，直到将block1发送完毕。</li>
<li>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。</li>
<li>Client收到host2发来的消息后，向NameNode发送消息，说我写完了。这样就真完成了。如图黄色粗实线</li>
<li>发送完block1后，再向host7、host8、host4发送block2，如图蓝色实线所示。</li>
<li>发送完block2后，host7、host8、host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。</li>
<li>Client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。</li>
</ol>
</li>
<li>分析：通过写过程，我们可以了解到<ol>
<li>写1T文件，我们需要3T的存储，3T的网络流量带宽。</li>
<li>在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。</li>
<li>挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。</li>
</ol>
</li>
</ol>
<h3 id="HDFS文件读取"><a href="#HDFS文件读取" class="headerlink" title="HDFS文件读取"></a>HDFS文件读取</h3><p>当文件读取：</p>
<ol>
<li>Client向NameNode发起文件读取的请求。</li>
<li>NameNode返回文件存储的block块信息、及其block块所在DataNode的信息。</li>
<li>Client读取文件信息。</li>
</ol>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220104177.png" alt="1"></p>
<p>如图所示，Client要从DataNode上，读取FileA。而FileA由block1和block2组成。读操作流程如下：</p>
<ol>
<li>Client向NameNode发送读请求。</li>
<li>NameNode查看Metadata信息，返回FileA的block的位置。<ol>
<li>block1:host2,host1,host3</li>
<li>block2:host7,host8,host4</li>
</ol>
</li>
<li>block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取。</li>
</ol>
<p>上面例子中，Client位于机架外，那么如果Client位于机架内某个DataNode上，例如,Client是host6。那么读取的时候，遵循的规律是：<strong>优选读取本机架上的数据。</strong></p>
<p>问题：如果读取block是按照先后顺序读，是否意味着在不同副本之间的读取是不平均的，没有考虑通过负载策略来提高读效率吗？</p>
<h3 id="备份数据的存放"><a href="#备份数据的存放" class="headerlink" title="备份数据的存放"></a>备份数据的存放</h3><p>备份数据的存放是HDFS可靠性和性能的关键。HDFS采用一种称为rack-aware的策略来决定备份数据的存放。</p>
<p>通过一个称为Rack Awareness的过程，NameNode决定每个DataNode所属rack id。</p>
<p>缺省情况下，一个block块会有三个备份：</p>
<ol>
<li>一个在NameNode指定的DataNode上</li>
<li>一个在指定DataNode非同一rack的DataNode上</li>
<li>一个在指定DataNode同一rack的DataNode上。</li>
</ol>
<p>这种策略综合考虑了同一rack失效、以及不同rack之间数据复制性能问题。</p>
<p>副本的选择：为了降低整体的带宽消耗和读取延时，HDFS会尽量读取最近的副本。如果在同一个rack上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么将首先尝试读本地数据中心的副本。</p>
<h3 id="Edits和Fsimage详解与合并流程"><a href="#Edits和Fsimage详解与合并流程" class="headerlink" title="Edits和Fsimage详解与合并流程"></a>Edits和Fsimage详解与合并流程</h3><h4 id="NameNode如何管理和存储元数据"><a href="#NameNode如何管理和存储元数据" class="headerlink" title="NameNode如何管理和存储元数据"></a>NameNode如何管理和存储元数据</h4><p>计算机中存储数据有两种：内存或磁盘 元数据存储磁盘: 存储磁盘无法面对客户端对元数据信息的任意的快速低延迟的响应，但是安全性高元数据存储内存：元数据存放内存，可以高效的查询以及快速响应客户端的查询请求，数据保存在内存，如果断电，内存中的数据全部丢失</p>
<p>因此，考虑上述两种存储方式的优缺点，HDFS采用了内存+磁盘的形式来管理元数据。即: NameNode(内存)+FsImage文件. 其中，NameNode文件维护了文件与数据块的映射表以及数据块与数据节点的映射表，比如，一个文件它被切分成了几个数据块，这些数据块分别存储在哪些datanode节点上。而Fsimage保存在磁盘上，为某一时刻下内存中元数据在本地磁盘的映射。就是在该时刻下，内存中元数据记录的所有文件块和目录，分别的状态，位于哪些datanode，各自的权限，各自的副本个数等 (可以通过查看Fsimage保存的内容可以看到上述信息)。因此，利用<strong>内存</strong>+<strong>磁盘</strong>的方式，内存中的元数据可以快速响应客户端的用户请求，而映射到磁盘中的元数据Fsimage可以实现安全性，防止数据的丢失。</p>
<p>而接下来的一个新问题是：磁盘和内存中的元数据如何进行划分。 即两个数据一摸一样，还是两个数据合并到一起才是一份完整的数据呢？</p>
<ul>
<li><strong>一模一样</strong>：client如果对元数据进行增删改操作，需要保证两个数据的一致。FsImage文件操作起来效率也不高，因为FsImage存储在磁盘中，对磁盘中的内容进行写入势必会增加很多的IO操作，也要占用CPU。此时，相当于每一次增删改操作，都需要对两个文件同时进行修改。</li>
<li><strong>两个合并&#x3D;完整数据</strong>：由于如果要将操作写入磁盘会降低运行效率。所以想法就是对于增删改操作，只有内存中的元数据进行响应，而不直接进行磁盘IO读写。此时，为了保证两个数据的一致性，NameNode就引入了以一个edits文件，该日志文件只能进行追加写入，以此来记录client的每次增删改操作。虽然此时仍然有IO流的操作，但是相比于每次将元数据内容写入Fsimage，edits日志文件的写入内容更少，效率更高。</li>
</ul>
<p>至此我们知道了HDFS元数据管理机制采用了内存+磁盘的形式，内存中的NameNode来快速响应客户端的查询请求，磁盘中的元数据作为备份，防止数据的丢失。同时，为了保证内存和磁盘中元数据的一致，hdfs采用了一个edits的日志文件，该文件记录了客户端对元数据的操作。利用edits+Fsimage的形式，就完成了对元数据的管理和存储。</p>
<h4 id="FsImage"><a href="#FsImage" class="headerlink" title="FsImage"></a>FsImage</h4><p>FsImage: 是namenode中关于元数据的镜像，一般称为检查点(checkpoing)，这里包含了HDFS文件系统所有目录以及文件相关信息（Block数量，副本数量，权限等信息）</p>
<p>在机器学习或这深度学习模型训练的过程中，为了防止异常中断，所以都会设定在某一具体时刻或者效果达到最好结果时，将模型参数都保存下来。这样，后续就可以在异常中断后利用来文件直接对模型的参数进行初始化赋值，而不用再重新进行训练。在模型训练中保存的文件也叫作checkpoint。因此这里HDFS的检查点checkpoint也可以同样来理解。即FsImage保存了某一时刻下元数据内的所有信息，这样，当HDFS异常中断或者程序启动时，就可以利用该检查点文件，来对元数据进行初始化，以此还原到异常中断或程序停止前的最新状态。</p>
<h4 id="Edits文件"><a href="#Edits文件" class="headerlink" title="Edits文件"></a>Edits文件</h4><p>存储了客户端对HDFS文件系统所有的更新操作记录，Client对HDFS文件系统所有的更新操作都会被记录到Edits文件中(不包括查询操作)</p>
<p>Client对HDFS的更新操作会更新内存中的元数据信息，而不会直接写入到FsImage文件中。那么为了保证内存和磁盘中元数据信息的一致性，就利用了edits文件来记录下所有的更新操作。<strong>edits文件记录的就是当原FsImage被载入内存后，Client又对元数据进行了哪些操作。</strong> 这样，只要在原FsImage中执行这些操作，对保存的元数据信息进行更新，就可以使得内存和磁盘汇总的元数据信息一致。通过此方法，即解决了为了保证一致性，要对FsImage直接进行写入的过程。这也是引入edits文件的关键原因。</p>
<h4 id="Fsimage与Edits的合并"><a href="#Fsimage与Edits的合并" class="headerlink" title="Fsimage与Edits的合并"></a>Fsimage与Edits的合并</h4><p>因为edits文件是一个只能追加写入的文件，在程序运行过程中会不断的记录客户端的更新操作。同时，根据上面对Edits中的介绍可知，引入载入的FsImage的元数据内容不是最新状态，所以只有在FsImage的内容上，执行edits文件中的更新操作，才能将FsImage的元数据更新为最新状态。此时，假设edits不断的进行追加写，当某一时刻需要NameNode重启时，此时NameNode会先将FsImage里面的内容映射到内存中，即相当于对元数据进行初始化，同时为了恢复到最新的状态，还需要在一条一条的执行edits中的记录。当edits文件非常大时，会导致NameNode启动过程非常慢，而在这段时间HDFS系统会处于<strong>安全模式</strong>，即保证了要将元数据恢复到最新后才能接收客户端请求。这显然是不符合用户要求的。因此，就需要设计能不能在NameNode运行的时候使得edits文件小一些，这样就能使启动过程加快。所以就要引入FsImage和Edits的合并过程。</p>
<p>合并过程的执行流程图：</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220104944.jpg" alt="2"></p>
<p>首先可以看出，除了NameNode节点外，为了进行合并，还引入了另一个SecondaryNameNode节点。SecondaryNameNode是HDFS架构中的一个组成部分，它是用来保存Namenode中对HDFS metadata的信息的备份而设定的。一般都是将SecondaryNamenode单独运行在一台机器上。</p>
<p>详细流程介绍如下： <strong>首先黑色字体代表的流程表示NameNode自身启动和运行时的流程，下面每个序号对应图中流程。</strong></p>
<p> 1）程序启动时，需要对元数据进行恢复。根据上述对FsImage和edits文件的已经知道，HDFS首先将Fsimage读入内存对元数据进行恢复，然后再读edits文件中的更新操作在恢复后的元数据上进行执行，使得此时的NameNode中保存的是停止前的最新状态。 </p>
<p>2、3）当有客户端执行增删改查操作时，HDFS会记录其中的增删改操作到edits文件中，这样就避免了直接对Fsimage文件的IO操作。 </p>
<ol start="4">
<li>内存中保存的元数据执行客户端的增删改查操作。（FsImage在此阶段是不改变的）</li>
</ol>
<p><strong>粉色字体代表的流程表示SecondaryNameNode对文件的合并流程，下面每个序号对应图中流程。</strong> </p>
<p>1）SecondaryNameNode向NameNode发送请求，询问是否需要进行合并。在Hadoop中通过两个维度来控制是否需要Checkpoint，如图中所示：1) 到定时时间 2) Edits文件是否满或超过了设定的大小范围。</p>
<p>2）如果满足上面的触发条件，则开始下述执行合并流程。 </p>
<p>3、4）由于NameNode需要将此时的edits文件和FsImage文件发送到SecondaryNamenode，所以在NameNode节点上需要停止使用该edits文件，暂时将新的写操作写到一个新的文件比如edits_inprogress_002中，而将原先的edits_inprogress_001重命名为esits_001进行发送。这样，有inprogress标识的edits表示最新正在写入更新操作的文件，而没有该标识，且后面数字最大的edits文件，即表示最后一个已经合并的文件。比如在实际的文件夹下会生成以下文件：</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220105511.jpg" alt="img"></p>
<p>各文件名的含义即如上所述。</p>
<p>5）SecondaryNamenode通过HTTP GET方式从NameNode上获取到fsimage和edits文件，并下载到本地的相应目录下。然后，SecondaryNameNode将下载下来的fsimage载入内存，然后一条一条地执行edits文件中的各项更新操作，使得内存中的fsimage保存最新；<strong>这个过程就是edits和fsimage文件的合并。</strong>（是不是跟我们上面说的在NameNode启动时的载入过程很像。HDFS就是利用了另一台机器的资源来对FsImage进行更新，这样NameNode所在节点的资源就只专注响应对客户端的操作）。</p>
<p>6）经过合并阶段之后，FsImage的内容即进行了更新，此时并不与NameNode中的元数据内容一致，相差的仍然是edits_inprogress_002中写入的更新操作。</p>
<p>7、8）SecondaryNameNode会通过post方式将新的FsImage文件发送到NameNode节点上。NameNode将接收到的新的fsimage替换旧的fsimage文件，同时将edit_inprogress_002文件来记录合并后续的更新操作。通过这个过程，edits就变小了。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>通过上面的描述可以总结几点。</p>
<ol>
<li>为了解决元数据备份的问题，HDFS采用了edits+FsImage的策略，edits文件保存了客户端的更新操作，Fsimage保存了元数据的具体内容。HDFS在运行时将客户端的操作响应在内容中的元数据上，同时将所有的更新操作写入edits文件，避免了直接对FsImage进行操作造成效率降低</li>
<li>HDFS为了解决edits不断追加写入过大的问题，采用了edits与FsImage合并的策略。即edits文件不是一直在原文件中进行写入的，而是在一定时间或者条件后，就把该时间段内客户端的更新操作同步到FsImage文件中，来更新FsImage文件的内容。这样，新的edits文件就可以只记录合并之后的更新操作，从而减小了edits文件的大小。</li>
<li>HDFS通过引入SecondaryNameNode来实现上述过程，利用了一台独立的机器资源来处理合并流程。所谓edits与FsImage的合并，其实就是在SecondaryNamenode内存中，将edits中记录一条一条的在fsiamge中执行，来更新Fsimage的内容。同时，可以知道，SecondaryNameNode获取到的FsImage不是最新的，因为在它从NameNode下载edits和FsImage文件的时候，新的更新操作已经写到新的edits文件里去了（比如这里的edits_inprogress_002）。而这些更新在SecondaryNamenode是没有同步到的。</li>
<li>edits和FsImage利用了文件命名格式来标识各自的最新文件。如fsiamge和edits文件图中所示，edits文件以edits_开头，后面跟一个txid范围端，并且多个edit log之间首尾相连，正在使用的edits文件名字为edits_inprogress_txid。该路径下还会保存两个FsImage文件（<code>dfs.namenode.num.checkpoints.retained</code>在namenode上保存的fsimage个数的默认配置，超过的被删除，默认保存两个）。文件格式为fsimage_txid，txid与edits中的txid对应，表示该fsimage加载的是哪一个edits。比如从图中可以看出，此时的fsimage已经加载到尾数为1545的edits文件内容</li>
</ol>
<h4 id="其它补充"><a href="#其它补充" class="headerlink" title="其它补充"></a>其它补充</h4><p>实际查看FsImage文件的内容时，可以发现Fsimage中是没有记录块所对应DataNode的。比如下图所示：</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220105313.jpg" alt="img"></p>
<p>在内存元数据中是有记录块所对应的datanode信息，但是fsimage中就剔除了这个信息；HDFS集群在启动的时候会加载image以及edits文件，block对应的dn信息都没有记录，集群启动时会有一个安全模式（safemode）,安全模式就是为了让datanode汇报自己当前所持有的block信息给nn来补全元数据。后续每隔一段时间datanode都要汇报自己持有的block信息。因为即使fsimage中记录了datanode信息，但是在恢复元数据的过程中，可能某些datanode节点出现了问题。所以，其实无论FsImage中是否对block的dn信息进行了记录，恢复的时候都是需要dn来汇报自己持有的block信息，这样才是最真实和安全的。</p>
<h2 id="二、实验代码及命令分析"><a href="#二、实验代码及命令分析" class="headerlink" title="二、实验代码及命令分析"></a>二、实验代码及命令分析</h2><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><p>hadoop fs 具体命令 OR hdfs dfs 具体命令</p>
<p>两个是完全相同的。</p>
<h3 id="命令大全"><a href="#命令大全" class="headerlink" title="命令大全"></a>命令大全</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-chgrp [-R] GROUP PATH...]</span><br><span class="line">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-head &lt;file&gt;]</span><br><span class="line">	[-help [cmd ...]]</span><br><span class="line">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">	[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-stat [format] &lt;path&gt; ...]</span><br><span class="line">	[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">	[-test -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value for a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides &#x27;fs.defaultFS&#x27; property from configurations.</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general command line syntax is:</span><br><span class="line">command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<h3 id="常见操作"><a href="#常见操作" class="headerlink" title="常见操作"></a>常见操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">help</span> 输出这个命令的参数</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -help rm</span><br><span class="line">-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ... :</span><br><span class="line">  Delete all files that match the specified file pattern. Equivalent to the Unix</span><br><span class="line">  command &quot;rm &lt;src&gt;&quot;</span><br><span class="line">                                                                                 </span><br><span class="line">  -f          If the file does not exist, do not display a diagnostic message or </span><br><span class="line">              modify the exit status to reflect an error.                        </span><br><span class="line">  -[rR]       Recursively deletes directories.                                   </span><br><span class="line">  -skipTrash  option bypasses trash, if enabled, and immediately deletes &lt;src&gt;.  </span><br><span class="line">  -safely     option requires safety confirmation, if enabled, requires          </span><br><span class="line">              confirmation before deleting large directory with more than        </span><br><span class="line">              &lt;hadoop.shell.delete.limit.num.files&gt; files. Delay is expected when</span><br><span class="line">              walking over large directory recursively to count the number of    </span><br><span class="line">              files to be deleted before the confirmation. </span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">mkdir</span> 新建文件夹</span></span><br><span class="line">[huang@hadoop102 ~]$ hadoop fs -mkdir /sanguo</span><br><span class="line">[huang@hadoop102 ~]$ cd /opt/module/hadoop-3.1.3/</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ vi shuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-moveFromLocal 从本地剪切粘贴到HDFS</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -moveFromLocal ./shuguo.txt /sanguo</span><br><span class="line">2021-06-30 13:55:50,370 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 212</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 bin</span><br><span class="line">drwxrwxr-x. 4 huang huang   4096 6月  28 11:54 data</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 huang huang 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">drwxrwxr-x. 3 huang huang   4096 6月  29 20:38 logs</span><br><span class="line">-rw-rw-r--. 1 huang huang  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 share</span><br><span class="line">drwxrwxr-x. 2 huang huang   4096 6月  28 12:13 wcinput</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ vi weiguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-copyFromLocal 从本地文件系统拷贝到HDFS路径去</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -copyFromLocal ./weiguo.txt /sanguo</span><br><span class="line">2021-06-30 13:57:22,614 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 216</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 bin</span><br><span class="line">drwxrwxr-x. 4 huang huang   4096 6月  28 11:54 data</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 huang huang 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">drwxrwxr-x. 3 huang huang   4096 6月  29 20:38 logs</span><br><span class="line">-rw-rw-r--. 1 huang huang  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 share</span><br><span class="line">drwxrwxr-x. 2 huang huang   4096 6月  28 12:13 wcinput</span><br><span class="line">-rw-rw-r--. 1 huang huang      7 6月  30 13:56 weiguo.txt</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ vi wuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-put 等同于copyFromLocal，生产环境更习惯用put</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -put ./wuguo.txt /sanguo</span><br><span class="line">2021-06-30 13:58:16,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ vi liubei.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-appendFile 追加一个文件到已经存在的文件末尾</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile ./liubei.txt /sanguo/shuguo.txt</span><br><span class="line">2021-06-30 14:00:23,765 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-copyToLocal 从HDFS拷贝到本地</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/shuguo.txt ./</span><br><span class="line">2021-06-30 14:01:16,117 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 228</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 bin</span><br><span class="line">drwxrwxr-x. 4 huang huang   4096 6月  28 11:54 data</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 huang huang 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang      7 6月  30 13:59 liubei.txt</span><br><span class="line">drwxrwxr-x. 3 huang huang   4096 6月  29 20:38 logs</span><br><span class="line">-rw-rw-r--. 1 huang huang  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 huang huang     14 6月  30 14:01 shuguo.txt</span><br><span class="line">drwxrwxr-x. 2 huang huang   4096 6月  28 12:13 wcinput</span><br><span class="line">-rw-rw-r--. 1 huang huang      7 6月  30 13:56 weiguo.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang      6 6月  30 13:58 wuguo.txt</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ vi shuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-get 等同于copyToLocal，生产环境更喜欢用get</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt </span><br><span class="line">2021-06-30 14:05:26,350 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 232</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 bin</span><br><span class="line">drwxrwxr-x. 4 huang huang   4096 6月  28 11:54 data</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 huang huang   4096 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 huang huang 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang      7 6月  30 13:59 liubei.txt</span><br><span class="line">drwxrwxr-x. 3 huang huang   4096 6月  29 20:38 logs</span><br><span class="line">-rw-rw-r--. 1 huang huang  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 huang huang   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 huang huang   4096 9月  12 2019 share</span><br><span class="line">-rw-r--r--. 1 huang huang     14 6月  30 14:05 shuguo2.txt</span><br><span class="line">-rw-r--r--. 1 huang huang     14 6月  30 14:01 shuguo.txt</span><br><span class="line">drwxrwxr-x. 2 huang huang   4096 6月  28 12:13 wcinput</span><br><span class="line">-rw-rw-r--. 1 huang huang      7 6月  30 13:56 weiguo.txt</span><br><span class="line">-rw-rw-r--. 1 huang huang      6 6月  30 13:58 wuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">ls</span> 显示目录信息</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /sanguo</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   3 huang supergroup         14 2021-06-30 14:00 /sanguo/shuguo.txt</span><br><span class="line">-rw-r--r--   3 huang supergroup          7 2021-06-30 13:57 /sanguo/weiguo.txt</span><br><span class="line">-rw-r--r--   3 huang supergroup          6 2021-06-30 13:58 /sanguo/wuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">cat</span> 显示文件内容</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -cat /sanguo/wuguo.txt</span><br><span class="line">2021-06-30 14:06:27,858 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">wuguo</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">chgrp</span>，-<span class="built_in">chmod</span>，-<span class="built_in">chown</span>：linux文件系统中的用法一样，修改文件所属权限</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -chmod 666 /sanguo/shuguo.txt</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -chown huang:huang /sanguo/shuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">mkdir</span> 创建路径</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /jinguo</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">cp</span> 从HDFS的一个路径拷贝到HDFS的另一个路径</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -cp /sanguo/shuguo.txt /jinguo</span><br><span class="line">2021-06-30 14:09:11,407 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">2021-06-30 14:09:11,512 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">mv</span> 在HDFS目录中移动文件</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/wuguo.txt /jinguo</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/weiguo.txt /jinguo</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">tail</span> 显示一个文件的末尾1kb的数据</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -tail /jinguo/shuguo.txt</span><br><span class="line">2021-06-30 14:11:43,198 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br><span class="line">shuguo</span><br><span class="line">liubei</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">rm</span> 删除文件或文件夹</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -rm /sanguo/shuguo.txt</span><br><span class="line">Deleted /sanguo/shuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">rm</span> -r 递归删除目录及目录里面内容</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /sanguo</span><br><span class="line">Deleted /sanguo</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-<span class="built_in">du</span> 统计文件夹的大小信息</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -du -s -h /jinguo</span><br><span class="line">27  81  /jinguo</span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -du -h /jinguo</span><br><span class="line">14  42  /jinguo/shuguo.txt</span><br><span class="line">7   21  /jinguo/weiguo.txt</span><br><span class="line">6   18  /jinguo/wuguo.txt</span><br><span class="line"><span class="meta">#</span><span class="language-bash">-setrep 设置HDFS中文件的副本数量</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">这里得看DataNode的数量，因为目前只有3台设备，最多就3个副本，只有节点数增加到10副本数才能达到10</span></span><br><span class="line">[huang@hadoop102 hadoop-3.1.3]$ hadoop fs -setrep 10 /jinguo/shuguo.txt</span><br><span class="line">Replication 10 set: /jinguo/shuguo.txt</span><br></pre></td></tr></table></figure>

<h2 id="三、实验结果"><a href="#三、实验结果" class="headerlink" title="三、实验结果"></a>三、实验结果</h2><p>通过web端可以查看到创建的文件夹：</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220106739.png" alt="image-20210630143235900"></p>
<p>这个也可以shuguo.txt的副本数为10:</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220106740.png" alt="image-20210630143324158"></p>
<p>文件信息：</p>
<p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220106741.png" alt="image-20210630143802614"></p>
<h2 id="四、实验遇到的问题及解决方法"><a href="#四、实验遇到的问题及解决方法" class="headerlink" title="四、实验遇到的问题及解决方法"></a>四、实验遇到的问题及解决方法</h2><h3 id="Hadoop102-9870-访问不了"><a href="#Hadoop102-9870-访问不了" class="headerlink" title="Hadoop102:9870 访问不了"></a>Hadoop102:9870 访问不了</h3><p><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202112220106742.png" alt="image-20210630144104874"></p>
<p>反复排查后，发现主机mac上的hosts设置错了，倒是hadoop102对应的ip找不到。</p>
<p>解决方案：</p>
<p>重新设置hosts即可</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">MingwHuang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-2%20HDFS/">http://example.com/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-2%20HDFS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">MingwHuang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202202172251674.png" target="_blank"><img class="post-qr-code-img" src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202202172251674.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202202172251973.png" target="_blank"><img class="post-qr-code-img" src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202202172251973.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-3%20MapReduce/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Lab-3 MapReduce</div></div></a></div><div class="next-post pull-right"><a href="/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-1%20Hadoop%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Lab-1 Hadoop环境安装</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-1%20Hadoop%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/" title="Lab-1 Hadoop环境安装"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-22</div><div class="title">Lab-1 Hadoop环境安装</div></div></a></div><div><a href="/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-3%20MapReduce/" title="Lab-3 MapReduce"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-22</div><div class="title">Lab-3 MapReduce</div></div></a></div><div><a href="/2021/12/22/%E5%A4%A7%E6%95%B0%E6%8D%AE/Lab-4%20%E5%A4%9A%E7%89%A9%E7%90%86%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="Lab-4 多物理机环境搭建"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-22</div><div class="title">Lab-4 多物理机环境搭建</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">MingwHuang</div><div class="author-info__description">朝花夕拾 聊以记之</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">40</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Lab-2-Hadoop-HDFS-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">Lab-2 Hadoop(HDFS)分布式文件系统基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90"><span class="toc-number">1.1.</span> <span class="toc-text">一、实验原理分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.1.</span> <span class="toc-text">HDFS是什么</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E4%BC%98%E7%82%B9"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">HDFS优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E7%BC%BA%E7%82%B9"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">HDFS缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">HDFS架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS-client"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">HDFS client</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NameNode"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">NameNode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataNode"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">DataNode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Secondary-NameNode"><span class="toc-number">1.1.2.4.</span> <span class="toc-text">Secondary NameNode</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5"><span class="toc-number">1.1.3.</span> <span class="toc-text">HDFS文件写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96"><span class="toc-number">1.1.4.</span> <span class="toc-text">HDFS文件读取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%87%E4%BB%BD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E6%94%BE"><span class="toc-number">1.1.5.</span> <span class="toc-text">备份数据的存放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Edits%E5%92%8CFsimage%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%90%88%E5%B9%B6%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.6.</span> <span class="toc-text">Edits和Fsimage详解与合并流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NameNode%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E5%92%8C%E5%AD%98%E5%82%A8%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">NameNode如何管理和存储元数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FsImage"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">FsImage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Edits%E6%96%87%E4%BB%B6"><span class="toc-number">1.1.6.3.</span> <span class="toc-text">Edits文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Fsimage%E4%B8%8EEdits%E7%9A%84%E5%90%88%E5%B9%B6"><span class="toc-number">1.1.6.4.</span> <span class="toc-text">Fsimage与Edits的合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.1.6.5.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E8%A1%A5%E5%85%85"><span class="toc-number">1.1.6.6.</span> <span class="toc-text">其它补充</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E5%8F%8A%E5%91%BD%E4%BB%A4%E5%88%86%E6%9E%90"><span class="toc-number">1.2.</span> <span class="toc-text">二、实验代码及命令分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8"><span class="toc-number">1.2.2.</span> <span class="toc-text">命令大全</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">常见操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.3.</span> <span class="toc-text">三、实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%AE%9E%E9%AA%8C%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">四、实验遇到的问题及解决方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop102-9870-%E8%AE%BF%E9%97%AE%E4%B8%8D%E4%BA%86"><span class="toc-number">1.4.1.</span> <span class="toc-text">Hadoop102:9870 访问不了</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/02/22/Chaos/Chaos/" title="无题"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/02/22/Chaos/Chaos/" title="无题">无题</a><time datetime="2022-02-22T13:55:23.925Z" title="发表于 2022-02-22 21:55:23">2022-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/20/Mac-%E6%B7%B1%E5%BA%A6%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/" title="Mac 深度使用技巧"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mac 深度使用技巧"/></a><div class="content"><a class="title" href="/2022/02/20/Mac-%E6%B7%B1%E5%BA%A6%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/" title="Mac 深度使用技巧">Mac 深度使用技巧</a><time datetime="2022-02-20T14:38:23.000Z" title="发表于 2022-02-20 22:38:23">2022-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/15/%E9%9D%A2%E8%AF%95/MySQL/" title="MySQL"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MySQL"/></a><div class="content"><a class="title" href="/2022/02/15/%E9%9D%A2%E8%AF%95/MySQL/" title="MySQL">MySQL</a><time datetime="2022-02-15T10:46:58.000Z" title="发表于 2022-02-15 18:46:58">2022-02-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/14/Chaos/%E6%90%AD%E5%BB%BA%E6%A2%AF%E5%AD%90/" title="搭建梯子"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="搭建梯子"/></a><div class="content"><a class="title" href="/2022/01/14/Chaos/%E6%90%AD%E5%BB%BA%E6%A2%AF%E5%AD%90/" title="搭建梯子">搭建梯子</a><time datetime="2022-01-14T13:13:14.000Z" title="发表于 2022-01-14 21:13:14">2022-01-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/29/Chaos/%E9%98%BF%E9%87%8C%E4%BA%91+Typora+PicGo+Github+Hexo+Next%E4%B8%BB%E9%A2%98%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="阿里云+Typora+PicGo+Github+Hexo+Next主题搭建个人博客"><img src="https://typora-mingwhuang.oss-cn-shenzhen.aliyuncs.com/typora/202202172217123.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="阿里云+Typora+PicGo+Github+Hexo+Next主题搭建个人博客"/></a><div class="content"><a class="title" href="/2021/12/29/Chaos/%E9%98%BF%E9%87%8C%E4%BA%91+Typora+PicGo+Github+Hexo+Next%E4%B8%BB%E9%A2%98%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="阿里云+Typora+PicGo+Github+Hexo+Next主题搭建个人博客">阿里云+Typora+PicGo+Github+Hexo+Next主题搭建个人博客</a><time datetime="2021-12-29T03:34:23.000Z" title="发表于 2021-12-29 11:34:23">2021-12-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By MingwHuang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><span class="footer-separator">|</span><a href="https://beian.miit.gov.cn/" target="_blank">赣ICP备2022001353号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>